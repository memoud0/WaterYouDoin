Implementation Checklist (tick as you go)
- [ ] Align normalization + regex between TS and Python. (Plan: L46)
- [ ] Fix inference math consistency (softmax or OVR + sigmoid). (Plan: L49)
- [ ] Add TS/Python feature parity test. (Plan: L52)
- [ ] Build 2k-5k dataset + labeling guide + golden set. (Plan: L58, L149, L159)
- [ ] Run full-pipeline eval (heuristics + ML blend). (Plan: L55)
- [ ] Export model weights and verify schema in extension. (Plan: L64)
- [ ] Consolidate storage key to "waterYouDoinStats". (Plan: L71)
- [ ] Expose decision/metrics payloads to frontend (DECISION + METRICS_UPDATE). (Plan: L67, L102, L114)
- [ ] Add hybrid prompt segmentation + aggregation. (Plan: L99)
- [ ] Re-tune thresholds to minimize LOW_VALUE false positives. (Plan: L61)
- [ ] Align severity scoring with narrative (factual redirects positive). (Plan: L80)
- [ ] Confirm target metrics are met (per-class precision/recall). (Plan: L143)

Repository ML + Core Review: Findings and Alignment Plan

Summary
This repository uses heuristics as the primary classifier and an offline-trained ML model as a fallback. The current ML pipeline exists, but feature parity and inference alignment are off, which limits how much the extension can benefit from ML.

ML Findings (ml/)
- Feature parity is broken: `ml/featurize.py` strips punctuation and apostrophes, while `extension/core/utils/text.ts` keeps internal punctuation. This changes tokenization and regex matches.
- Regex divergence: Python `startsWithWh` includes "whats/whos/whens", TS does not. Python cannot detect code fences/symbols because punctuation is removed.
- Inference mismatch: exported model is multinomial softmax, but `extension/core/classify/model.ts` uses per-class sigmoid.
- Evaluation is optimistic: `ml/evaluate.py` reports on the full dataset (no holdout).
- Dataset is small/synthetic (394 rows), LOW_VALUE-heavy, short prompts only, with duplicates.

Extension/Core Findings (extension/)
- `normalize()` retains internal punctuation, so “what’s” vs “whats” behave differently; `RE_WH_START` misses “whats”.
- Heuristics are strong and ML is only blended when heuristic max < 0.75; ML impact may be minimal on borderline cases.
- Low-value handling relies on a small exact list + length/emoji checks; the dataset contains many low-value variants not in `LOW_VALUE_EXACT`.
- `model_weights.json` metadata says softmax, but runtime uses sigmoid. This creates inconsistent probabilities and rankings.
- Two storage modules exist with different keys (`storage.ts` vs `schema.ts`), which can split metrics/state.
- Messaging types return `STATS` as `unknown`, and decisions do not expose `classification/confidence/signals` for UI/metrics.
- Several core/extension files are empty placeholders (frontend-owned for now): `extension/manifest.json`, `extension/scripts/content.ts`, `extension/scripts/install.ts`, `extension/core/config/constants.ts`, `extension/core/config/sites.ts`, `extension/core/redirect/routes.ts`, `extension/core/storage/migrate.ts`, `extension/core/utils/time.ts`, `extension/data/vocab.json`, `extension/data/prompt_examples.json`.
- `background.ts` always uses `DOGPILE` and ignores `stats.settings.searchProvider`.
- Severity scoring increases with factual redirects (which are actually positive behaviors), possibly conflicting with the mascot narrative.

Alignment Gaps (Core ↔ ML)
- Different normalization/tokenization rules.
- Different regex vocab (WH-start, code tokens, compare words).
- Different inference math.
- No shared test that validates feature extraction parity across TS + Python.
- No shared evaluation harness for “full pipeline” (heuristics + ML blend).

Plan (Short-Term: implement now)
Core + ML Alignment
1) Single source of truth for normalization + regex:
   - Pick one canonical normalization and port it to both TS and Python.
   - Use the same regex sets for WH-start, code tokens, compare/build/error/lookup terms.
2) Fix inference math consistency:
   - Option A: TS applies softmax over logits (preferred if keeping multinomial LogReg).
   - Option B: train one-vs-rest and keep sigmoid inference.
3) Add parity tests:
   - Small script that runs a fixed prompt list through both TS and Python feature extractors and asserts identical outputs.
   - Include “whats”, code fences, emojis, short acronyms, and long prompts.
4) Create a full-pipeline evaluation script:
   - Simulate classification with heuristics + ML blend (same thresholds as runtime).
   - Output confusion matrix and top misclassifications.
5) Expand dataset to a “starter” realistic size (local-only; dataset does not ship to extension):
   - 2k–5k prompts with balanced classes and clear labeling rules.
   - Include real-world variants: typos, slang, code blocks, stack traces, mixed intent, and short technical acronyms.
6) Re-tune thresholds after new data:
   - Adjust `DEFAULT_THRESHOLDS` or ML blend weight to reduce LOW_VALUE false positives.
   - Prioritize minimizing LOW_VALUE false positives (blocking valid prompts is the highest cost).
7) Export model weights and verify schema in extension:
   - Regenerate `extension/data/model_weights.json` after training.
   - Verify labels, feature order, and inference math align with runtime.
8) Expose a stable backend interface for the UI:
   - Add a typed `DecisionMsg` payload with `classification`, `confidence`, `signals`, and optional `probs`.
   - Add a `ClassificationEvent` or `METRICS_UPDATE` message so UI can display live metrics without polling.
   - Document payloads and frontend steps (see "Frontend Integration Steps" below).
9) Consolidate storage and metrics:
   - Use a single storage module/key and keep schema + migration in one place.
   - Canonical storage key: "waterYouDoinStats".
   - Record per-class counts and “ml_used” counts for transparency.
10) Wire runtime config and extension wiring:
   - Fill `manifest.json` with content script + background/service worker + permissions (`storage`, `tabs`, site matches).
   - Define supported sites in `config/sites.ts` and route helpers in `redirect/routes.ts`.
   - Implement `content.ts` intercept + debounce and `install.ts` (if needed for onboarding).
   - Hold until frontend delivers content-script implementation details.
11) Align awareness metrics with narrative:
   - Factual redirects are positive (user chooses non-AI lookup); severity should not increase for redirects.
   - If user proceeds to AI after a factual alternative is offered, that should increase severity.
   - Revisit severity scoring so positive actions (factual redirects, try-myself) reduce severity.
   - Add prompt-quality and “intentionality” signals to drive UI messaging.

Plan (Mid-Term: still feasible for hackathon scale)
ML Improvements
- Add a stable train/val/test split and report metrics on test only.
- Add active-learning loop: use `ml/artifacts/misclassified.csv` to seed new labels.
- Introduce simple additional features (length buckets, punctuation density, URL/email detection, digit ratio).
- Write a lightweight labeling guide to keep class boundaries consistent.

Core Improvements
- Expand low-value lexicon + add whitelist for short meaningful tokens (e.g., jwt, http, tcp, css, sql).
- Add a small “golden set” test in `extension/core/classify/__tests__` that mirrors ML evaluation prompts.
- Improve “whats” handling: normalize contractions (whats → what is) or add regex for `^whats\b`.
- Validate feature vector order at runtime (assert size + names).
- Add a smoke test that confirms classification never triggers network calls (local-only guarantee).
- Add “hybrid prompt” handling: split by sentence/newline, classify segments, and aggregate (any REASONING → REASONING; else any FACTUAL → FACTUAL; else LOW_VALUE).
- Surface a “prompt quality” score (e.g., clarity/structure) to power UI nudges without blocking.

Frontend Integration Steps (plan only)
1) Content script:
   - On input change or submit, send `PROMPT_SUBMIT` with `prompt`, `pageUrl`, `timestamp`, and optional `context`.
   - Handle `DECISION` and render: action, classification badge, confidence meter, and signals tooltip.
2) Background script:
   - On `PROMPT_SUBMIT`, run `factualOrNot()` and build the full `DECISION` payload.
   - Update metrics counters and include `metricsSnapshot` in the `DECISION`.
   - Optionally broadcast `METRICS_UPDATE` after each metrics update.
3) UI (popup/options/nudge):
   - Subscribe to `METRICS_UPDATE` and refresh dashboard without polling.
   - Fallback to `GET_STATS` on initial load.

Expected Payloads (contract)
PROMPT_SUBMIT (content -> background)
{
  "type": "PROMPT_SUBMIT",
  "prompt": "string",
  "pageUrl": "string",
  "timestamp": 0,
  "context": { "siteId": "string", "inputId": "string", "selection": "string", "modelHint": "string" }
}

DECISION (background -> content/UI)
{
  "type": "DECISION",
  "action": "ALLOW|BLOCK_LOW_VALUE|SHOW_NUDGE|REDIRECT",
  "classification": "FACTUAL|LOW_VALUE|REASONING",
  "confidence": 0.0,
  "signals": ["string"],
  "probs": { "FACTUAL": 0.0, "LOW_VALUE": 0.0, "REASONING": 0.0 },
  "segments": [{ "text": "string", "classification": "FACTUAL|LOW_VALUE|REASONING", "confidence": 0.0 }],
  "metricsSnapshot": { "see StoredStats" }
}

METRICS_UPDATE (background -> UI)
{
  "type": "METRICS_UPDATE",
  "data": { "see StoredStats" }
}

Consistency Checklist (Done when all are true)
- Feature extraction outputs match for the same prompt in TS and Python.
- Model inference math matches training setup.
- A shared “golden set” is used for ML training + core tests.
- The extension log output can explain why a prompt was classified (signals from heuristics + ML).
- LOW_VALUE false positives stay below an agreed threshold on the test set (e.g., <2–3% of non-low-value prompts).
- Target metrics (suggested for hackathon):
  - FACTUAL precision >= 0.90, recall >= 0.85
  - REASONING precision >= 0.90, recall >= 0.85
  - LOW_VALUE precision >= 0.95, recall >= 0.85

Maintenance Notes
- If you edit this file, update checklist line references at the top.

Labeling Guide (Draft)
- FACTUAL: single-lookup or definition questions; short factual answers; no multi-step reasoning. Examples: “what is tcp”, “syntax for sql join”, “capital of japan”.
- LOW_VALUE: acknowledgements, greetings, filler, emoji/punct-only, or duplicates with no intent to learn. Examples: “ok”, “thanks”, “lol”, “??”.
- REASONING: asks for analysis, debugging, design, comparison, step-by-step, or code generation. Examples: “compare oauth vs saml and explain tradeoffs”, “debug this error…”.
Rules:
- If any part requires reasoning, label REASONING.
- If ambiguous between LOW_VALUE and FACTUAL, pick FACTUAL (avoid costly false positives).
- Short technical acronyms are FACTUAL (jwt, tcp, css, sql, grpc, etc.).
- Use REASONING when the user asks “how/why/design/debug/compare/implement”.

Dataset Seed List + Expansion Strategy (2k–5k prompts)
- FACTUAL templates:
  - “what is <protocol/term>”, “what does <acronym> stand for”, “meaning/definition of <term>”
  - “syntax for <language feature>”, “http <status> meaning”, “port <n> used for”
  - “capital/timezone/population of <place>”, “release date of <tool>”
- REASONING templates:
  - “design/architect <system>”, “debug <error>”, “optimize <query/code>”
  - “compare <tech A> vs <tech B>”, “implement <data structure/feature>”
  - “explain why/how <behavior> happens”
- LOW_VALUE templates:
  - greetings, thanks, confirmations, emojis, punctuation-only, “test/testing”
- Hybrid prompts (label REASONING):
  - “thanks, now help me debug…”, “ok, compare X vs Y and explain”
- Noise/edge cases:
  - typos (“whats”, “wut is”), slang, mixed casing, short acronyms (“jwt exp?”, “grpc?”)
Guidance:
- Use template expansion + manual review to reach target size.
- Maintain class balance and include 20–30% long prompts with code/stack traces.
- Keep a fixed 200–300 prompt “golden set” held out for tests/metrics.
