ML offline training and ship improved model_weights.json.

Train a lightweight classifier offline (logistic regression or naive bayes), evaluate it, then export weights into:
extension/data/model_weights.json

Everything under /ml is offline only and should not ship in the extension bundle.

Blueprint pipeline structure:
ml/dataset/prompts.csv (prompt, label)


ml/featurize.py (must match features.ts)


ml/train.py (train logreg/nb)


ml/evaluate.py (accuracy + precision/recall + confusion matrix)


ml/export_to_extension.py (writes weights JSON to extension folder)


Step-by-step instructions:

Build dataset


Create/expand ml/dataset/prompts.csv


Columns: prompt,label with labels in {FACTUAL, LOW_VALUE, REASONING}【turn2file2†McHacks_Blueprint.pdf†L69-L77】


Implement featurization in Python


ml/featurize.py must match the TypeScript features.ts exactly【turn2file2†McHacks_Blueprint.pdf†L79-L83】


Key: same tokenization/normalization rules (especially for “whats” issue!)


Train


ml/train.py: logistic regression / naive bayes, export weights + bias per class【turn2file2†McHacks_Blueprint.pdf†L83-L93】


Evaluate


ml/evaluate.py: output accuracy, per-class precision/recall, confusion matrix【turn2file2†McHacks_Blueprint.pdf†L95-L106】


Export


ml/export_to_extension.py writes extension/data/model_weights.json (and optional vocab.json if used)【turn2file2†McHacks_Blueprint.pdf†L108-L116】


Wire weights into extension


Ensure build copies extension/data/model_weights.json into dist (blueprint mentions static copy during build).【turn2file2†McHacks_Blueprint.pdf†L33-L62】


Re-run tests


npm test must still pass


Add 10-20 "nasty" prompts that currently fail (e.g., "whats the capital of japan", "whats tcp", "k", "okkk", "???") and assert expected classes.

ML success criteria (what “good” looks like)
ML should mainly help borderline factual lookups that heuristics struggle with ("whats ...", short "syntax" queries, etc.)


ML should not override obvious REASONING prompts (debug/design/compare/implement)


ML should reduce LOW_VALUE false positives (short but meaningful prompts like "jwt exp?")

Known TODO list (high ROI)
Fix normalization for "whats"
Make normalize() convert:
whats -> what is
optionally whos -> who is
Or handle in heuristics regex (^whats\b)


Upgrade LOW_VALUE rules
Expand low-value lexicon + regex coverage (slang, emojis, repeats, punctuation-only)


Add a “short but meaningful” whitelist (jwt, http, tcp, css, api, sql, etc.)


Improve ML dataset for real usage
Add common real chat prompts using the dataset under ML folder, not just textbook ones


Balance classes (low-value can be overrepresented if you include too many greetings)

---
Core + ML alignment requirements (must-do)
- Single source of truth for normalization + regexes (TS + Python must match).
- Inference math must match training (softmax logits or OVR + sigmoid).
- Add a parity test: same prompt list must yield identical features in TS + Python.
- Add a full-pipeline eval: heuristics + ML blend, with confusion matrix.
- Ensure model export schema matches runtime (feature order, labels).

Dataset scope and labeling (hackathon target)
- Build a 2k-5k prompt dataset with balanced classes.
- Include realistic prompts: typos, slang, code blocks, stack traces, mixed intent.
- Keep a fixed 200-300 prompt "golden set" held out for tests.
- Labeling rules:
  - FACTUAL: single-lookup or definition questions (short factual answers).
  - LOW_VALUE: greetings/acknowledgements/emoji-only/duplicate filler.
  - REASONING: analysis, debugging, design, comparison, implementation.
  - If any part requires reasoning, label REASONING.
  - If ambiguous between LOW_VALUE and FACTUAL, pick FACTUAL.
  - Short technical acronyms are FACTUAL (jwt, tcp, css, sql, grpc).

Hybrid prompt handling (new feature)
- Split prompts by sentence/newline, classify each segment, aggregate:
  - If any segment is REASONING -> REASONING
  - Else if any segment is FACTUAL -> FACTUAL
  - Else -> LOW_VALUE
- Return per-segment classifications for UI transparency.

Metrics and awareness improvements
- Track per-class counts, ML usage, and prompt quality score.
- Expose a live metrics message so UI can update without polling.
- Ensure local-only guarantee (no network calls) with a smoke test.
- Consolidate storage so metrics/state are not split across modules.

Frontend interface (short JSON schema)
Decision response (background -> content/UI):
{
  "type": "DECISION",
  "action": "ALLOW|BLOCK_LOW_VALUE|SHOW_NUDGE|REDIRECT",
  "classification": "FACTUAL|LOW_VALUE|REASONING",
  "confidence": 0.0,
  "signals": ["string"],
  "probs": { "FACTUAL": 0.0, "LOW_VALUE": 0.0, "REASONING": 0.0 },
  "segments": [
    { "text": "string", "classification": "FACTUAL|LOW_VALUE|REASONING", "confidence": 0.0 }
  ]
}

Metrics update (background -> UI):
{
  "type": "METRICS_UPDATE",
  "data": {
    "today": { "factualRedirects": 0, "lowValueBlocks": 0, "reasoningNudges": 0, "avoidedAiCalls": 0 },
    "lifetime": { "avoidedAiCalls": 0 },
    "water": { "litersSavedDaily": 0, "litersSavedLifetime": 0 },
    "severity": { "score": 0, "mascotState": "SOLID|THINKING|MELTING|DROPLET" },
    "ml": { "usedCount": 0 }
  }
}

